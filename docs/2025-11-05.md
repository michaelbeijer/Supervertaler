# Supervertaler Qt Development Session - November 5, 2025

## Session Overview

This session focused on fixing the "Generate Prompts" feature in the Prompt Assistant, which was producing incomplete and incorrectly formatted Domain and Project Prompts.

---

## The Problem: Generated Prompts Were Incomplete

### Initial Symptoms
1. **Domain Prompt**: Only contained 2-3 short sentences instead of comprehensive 3-5 paragraphs
2. **Project Prompt**: Missing introductory paragraphs; only contained partial termbase table (sometimes truncated mid-table)
3. **Terminology**: Dialog used incorrect labels ("System Prompt" instead of "Domain Prompt", "Custom Instructions" instead of "Project Prompt")

### Example of Broken Output
- Domain Prompt: Just number formatting rules and figure references (~2 sentences)
- Project Prompt: Only termbase table, no context or closing paragraphs

---

## The Root Cause: Using Wrong Tool for the Job

### Discovery Process

**Multiple Failed Attempts:**
1. ✗ Improved logging and parsing logic
2. ✗ Added placeholder replacement for termbase
3. ✗ Programmatic termbase extraction (hybrid approach)
4. ✗ Increased `max_tokens` parameter
5. ✗ Adjusted prompt instructions to be more explicit

**The Real Problem:**
The code was using `client.translate()` - a method designed for **translation tasks** - for **creative text generation**. This was fundamentally wrong.

```python
# WRONG APPROACH (what we were doing):
ai_response = client.translate(
    text="",  # Empty text, generation is in prompt
    source_lang=source_lang,
    target_lang=target_lang,
    custom_prompt=full_custom_prompt,
    max_tokens=8000
)
```

The `translate()` method:
- Is optimized for translation output (source → target)
- Has built-in post-processing and cleaning
- Constrains the LLM's behavior for translation tasks
- Was never designed for complex text generation

---

## The Solution: Direct LLM API Calls

### What the Tkinter Version Does (Correctly)

The working tkinter version calls the LLM APIs **directly** with proper chat messages:

```python
# OpenAI Example
response = client.chat.completions.create(
    model=model_name,
    messages=[
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_prompt}
    ],
    temperature=0.4,
    max_tokens=8000
)
```

### Key Differences

| Aspect | Wrong (translate) | Right (direct API) |
|--------|-------------------|-------------------|
| Method | `client.translate()` | `client.chat.completions.create()` |
| Message Structure | Combined prompt | Separate system/user roles |
| Temperature | Default (0.3) | 0.4 (more creative) |
| Max Tokens | 8000 (but constrained) | 8000 (full flexibility) |
| Purpose | Translation | Text generation |
| Post-processing | Translation cleanup | Raw output |

---

## Implementation Details

### Direct API Calls for Each Provider

#### OpenAI
```python
from openai import OpenAI
openai_client = OpenAI(api_key=api_keys.get('openai'))

response = openai_client.chat.completions.create(
    model=model_name,
    messages=[
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_prompt}
    ],
    temperature=0.4,
    max_tokens=8000
)
ai_response = response.choices[0].message.content

# Check for truncation
if response.choices[0].finish_reason == 'length':
    log("⚠️ WARNING: Response truncated due to token limit!")
```

#### Claude (Anthropic)
```python
import anthropic
claude_client = anthropic.Anthropic(api_key=api_keys.get('claude'))

response = claude_client.messages.create(
    model=model_name,
    max_tokens=8000,
    temperature=0.4,
    system=system_prompt,
    messages=[{"role": "user", "content": user_prompt}]
)
ai_response = response.content[0].text

# Check for truncation
if response.stop_reason == 'max_tokens':
    log("⚠️ WARNING: Response truncated due to token limit!")
```

#### Gemini (Google)
```python
import google.generativeai as genai
genai.configure(api_key=api_keys.get('google'))
gemini_model = genai.GenerativeModel(model_name)

combined = system_prompt + "\n\n" + user_prompt
response = gemini_model.generate_content(
    combined,
    generation_config=genai.types.GenerationConfig(
        temperature=0.4,
        max_output_tokens=8000
    )
)
ai_response = response.text

# Check for truncation
if hasattr(response, 'candidates') and response.candidates:
    finish_reason = response.candidates[0].finish_reason
    if finish_reason == 1:  # FINISH_REASON_MAX_TOKENS
        log("⚠️ WARNING: Response truncated due to token limit!")
```

---

## The Hybrid Approach (Attempted but Abandoned)

### What We Tried

Before discovering the root cause, we attempted a hybrid approach:
1. **Programmatically extract** the termbase table from the analysis using regex
2. **Ask the AI** to generate Domain Prompt and Project Prompt intro/closing paragraphs with a placeholder
3. **Replace the placeholder** with the extracted termbase

### Code Example (Removed)
```python
# Extract termbase programmatically
table_pattern = r'\|.*en term.*\|.*nl equivalent.*\|.*Notes.*context.*\|'
table_match = re.search(table_pattern, analysis_text, re.IGNORECASE)
# ... extract full table ...

# AI generates prompts with placeholder
# "[TERMBASE TABLE WILL BE INSERTED HERE]"

# Replace placeholder
project_prompt_text = project_prompt_text.replace(
    "[TERMBASE TABLE WILL BE INSERTED HERE]",
    f"\n\n**Bilingual Termbase:**\n\n{termbase_table}\n\n"
)
```

### Why We Abandoned It

Once we fixed the root cause (using direct API calls), the hybrid approach was **unnecessary complexity**:
- Added regex parsing (potential failure point)
- Added placeholder replacement logic
- Solving symptoms, not the disease
- Tkinter works fine with pure AI - so should we

**Simpler is better.** Let the AI do what it's designed to do.

---

## Final Architecture: AI-Only Approach

### System Prompt (Instructions to LLM)

The system prompt contains detailed instructions for generating two prompts:

1. **DOMAIN PROMPT (Layer 2)**
   - Complete, comprehensive prompt (3-5 paragraphs minimum)
   - Language-agnostic (uses `{{SOURCE_LANGUAGE}}` and `{{TARGET_LANGUAGE}}` placeholders)
   - Covers: Professional context, translation role, task definition, output format, CAT tool tags, number formatting, domain guidelines

2. **PROJECT PROMPT (Layer 3)**
   - 2-3 introductory paragraphs of document-specific guidance
   - **COMPLETE termbase table** copied verbatim from the analysis
   - 2-3 closing paragraphs with examples and consistency rules

### Key Instruction for Termbase

```
**CRITICAL: termbase TABLE REQUIREMENTS**
- You MUST copy the ENTIRE bilingual termbase table from the analysis above
- Copy it VERBATIM - every single row, word-for-word
- The table header must be: | en term | nl equivalent | Notes / context |
- Include the separator line: |------------|--------------------|-----------------| 
- Then copy EVERY SINGLE ROW from the analysis termbase
- If there are 36 terms in the analysis, there must be 36 rows in your output
- DO NOT STOP until you've copied the LAST row of the termbase
```

### Response Format

```
---SYSTEM PROMPT---
[Full Domain Prompt text here, ready to copy-paste]

---CUSTOM INSTRUCTIONS---
[Full Project Prompt text here, ready to copy-paste]

---
```

Note: Uses old delimiter names ("SYSTEM PROMPT", "CUSTOM INSTRUCTIONS") for backward compatibility with parsing logic, but generates correct Layer 2/Layer 3 content.

---

## Lessons Learned

### 1. **Match the Tool to the Task**
- Translation method → translation tasks
- Chat completion → text generation
- Don't try to force one to do the other's job

### 2. **Look at What Works**
The tkinter version was working perfectly. We should have copied that approach **exactly** from the start, not tried to adapt it through a different abstraction layer.

### 3. **Root Cause vs. Symptoms**
We spent significant time addressing symptoms (parsing, truncation, placeholders) when the real problem was the fundamental approach.

### 4. **AI Models Are Different from Traditional Code**
You can't just wrap an LLM call in a generic interface and expect it to work for all use cases. Different tasks need different API configurations:
- **Translation**: Lower temperature, constrained output, post-processing
- **Creative generation**: Higher temperature, flexible output, raw results

### 5. **Simplicity Wins**
The hybrid approach added complexity to work around the wrong approach. The correct approach is simpler: just call the API correctly.

---

## Code Changes Summary

### Modified Files
- `modules/prompt_manager_qt.py`

### Key Changes

1. **Replaced `client.translate()` with direct API calls**
   - Lines ~2790-2854: Added provider-specific API call logic
   - Proper system/user message separation
   - `temperature=0.4` for creative generation
   - `max_tokens=8000` with truncation detection

2. **Removed hybrid approach**
   - Removed termbase extraction regex (~35 lines)
   - Removed placeholder replacement logic (~50 lines)
   - Removed `termbase_table` parameter from event system
   - Cleaned up method signatures

3. **Updated prompt instructions**
   - Restored original termbase copy instructions (from tkinter)
   - Removed placeholder-based approach
   - Emphasized "copy VERBATIM" requirement

---

## Testing Results

### Before Fix
- Domain Prompt: 2 sentences (~100 characters)
- Project Prompt: Partial termbase, no intro/closing paragraphs (~1500 characters)
- **Result**: Unusable

### After Fix
- Domain Prompt: Complete 5-paragraph prompt with all required sections (~2500+ characters)
- Project Prompt: 2-3 intro paragraphs + complete 36-term termbase + 2 closing paragraphs (~4000+ characters)
- **Result**: Perfect! ✅

---

## Why This Was Hard to Debug

1. **The abstraction seemed reasonable** - Using a universal `translate()` method for all LLM tasks seems logical
2. **It partially worked** - Got some output, just not complete output
3. **Multiple potential causes** - Truncation, parsing, token limits, instruction quality
4. **Working code existed** - But in a different codebase (tkinter) using a different approach
5. **The AI (Cursor Composer) got tunnel vision** - Kept trying to fix symptoms rather than questioning the fundamental approach

---

## Conclusion

**Final Status**: ✅ **WORKING PERFECTLY**

The "Generate Prompts" feature now produces complete, properly formatted Domain and Project Prompts with full termbase tables, matching the quality and completeness of the tkinter version.

**Key Takeaway**: Use the right tool for the job. Translation methods are for translation. Text generation needs direct chat completion API calls with proper message structure and creative temperature settings.

---

## Related Features Discussed (Not Implemented)

During the session, we also discussed (but did not implement) several other improvements:
- Bulk operations menu organization
- Project prompt settings persistence
- Configurable batch translation settings
- Startup layout improvements

These were background context but not the focus of this debugging session.

---

## Future Considerations

1. **Consider renaming or deprecating `client.translate()`** - If it's meant only for translation, make that explicit
2. **Add a `client.generate()` method** - For text generation tasks, wrapping the chat completion APIs
3. **Document the use cases** - Clear guidance on when to use which method
4. **Add type hints** - Make it clear what each method is designed for

---

*Session completed successfully at approximately 00:00 on November 6, 2025.*

